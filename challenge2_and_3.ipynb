{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd \n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from treeinterpreter import treeinterpreter as ti\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this binary prediction task, I have decided to use the decision tree classifier. The latter is indeed a 'white box' model, as it is highly interpretable and its functiong and choises for each data point can be deeply investigated and explained.\n",
    "Furthermore, the decision tree classifier is one of the best model for classification tasks. \n",
    "A decision tree is also capable of working with missing values, and this dataset contains a non-negligible number of them.\n",
    "\n",
    "Note: the request of challenge 2 specifies that the probability of an employee not to be loyal. I will provide the code for having the model to return a probability, and I will apply it on the predictions on the test set, on which the best model configuration will be tested. I will put it at the end of the document. \n",
    "The predictions that you will see on the validation set will be binary predictions: I am doing this for model comparison, and because the binary classification evaluation metrics that I'll be using, require binarized predictions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a custom function aimed at displayig the most importan metrics for a binary classification, together with a confusion matrix\n",
    "# The input for this function must be binary predictions\n",
    "\n",
    "def evaluate(y_pred, y_validation):\n",
    "    cm = confusion_matrix(y_validation, y_pred)\n",
    "    acc = accuracy_score(y_validation,y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    recall = tp / (tp + fn)\n",
    "    beta = 0.5\n",
    "\n",
    "    if tp == 0 and fp == 0:\n",
    "        precision = 0.0\n",
    "        F1 = 0.0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "        F1 = (1+beta**2)*((precision * recall)/((beta**2 * precision) + recall))\n",
    "        \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"{:>10} {:>10} {:>10}\".format(\"\", \"Predicted 0\", \"Predicted 1\"))\n",
    "    print(\"{:>10} {:>10} {:>10}\".format(\"Actual 0\", tn, fp))\n",
    "    print(\"{:>10} {:>10} {:>10}\".format(\"Actual 1\", fn, tp))\n",
    "    print(\"Recall:\", round(recall, 3))\n",
    "    print(\"Precision:\", round(precision, 3))\n",
    "    print(\"Accuracy:\", round(acc, 4))\n",
    "    print(\"F1 score:\", round(F1, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/leotasso3/Xtream_Tasso/main/datasets/employee-churn/churn.csv\")  # Reading the dataset\n",
    "\n",
    "# isolating the target variable, which is in the last column of the dataset\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# The predictors will be all the other variables, but for the city (explanation below) and the ID column \n",
    "x = df.iloc[:, 1:13]\n",
    "x=x.drop('city', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided to split the dataset into training (70%), validation (15%) and test set (15%). The training set will be indeed used for training the DT to build its nodes and leaves. \n",
    "I want to try different configuration of the model, starting from a baseline one with the (almost) untouched dataset, for then moving to hyperparameter optimization, PCA and feature selection. This is why the validation set is here. After training the model, I will test the performance of the latter on the validation data, and as I want to try different configuration, I will do it more than once. \n",
    "The metrics obtained on the validation set will be used for selecting the model configuration that yielded the best results. That model will then be tested on the test set: this will give the opportunity to evaluate the generalization of the model on unseen data.\n",
    "Without the validation set, testing different models configurations on the test set would could lead to overfitting, as doing this would basically mean to adapt the choice of the best model configuration on future data, which of course is not possible in reality. \n",
    "As the dataset is slightly imbalanced, I will apply stratification with respect to the target variable, so that the same proportion of the latter will be present in every set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the target variable in the training set: target\n",
      "0.0    0.750674\n",
      "1.0    0.249326\n",
      "Name: proportion, dtype: float64\n",
      "Distribution of the target variable in the validation set: target\n",
      "0.0    0.750682\n",
      "1.0    0.249318\n",
      "Name: proportion, dtype: float64\n",
      "Distribution of the target variable in the test set: target\n",
      "0.0    0.750522\n",
      "1.0    0.249478\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Set creation and stratification (the randon_state is for reproducibility purposes)\n",
    "x_train_temp, x_test, y_train_temp, y_test = train_test_split(x, y, test_size=0.15, random_state=42, stratify=y)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_temp, y_train_temp, test_size=0.18, random_state=42, stratify=y_train_temp)\n",
    "\n",
    "\n",
    "# Verification of stratification\n",
    "print(\"Distribution of the target variable in the training set:\", y_train.value_counts(normalize=True))\n",
    "print(\"Distribution of the target variable in the validation set:\", y_val.value_counts(normalize=True))\n",
    "print(\"Distribution of the target variable in the test set:\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the DecisionTreeClassifier won't work with 'object' as variable type (categorical), I will apply dummy encoding.\n",
    "This will increase the dimensionality of the dataset and could therefore increment the computational costs for the model training.\n",
    "For an eventual future dataset way larger than the one provided here, it is advisable to apply PCA or feature selection (which will in any case tested on this dataset as well).\n",
    "The reasons why I have decided to remove the 'city' column are mainly 2: \n",
    "-This variable has 73 classes, and dummy coding it would result in a severe increase in the dimensionality of the dataset, which of course is not positive either for computational costs and risk of overfitting\n",
    "-Stratify this variable could be very difficult: if there are very few people coming from one city, it could be that the relative coded column won't be present in all the 3 sets, which of course will lead to an error of model training or model prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = x_train.select_dtypes(include=['object']).columns.tolist() # Storing the categorical variable\n",
    "\n",
    "# Dummy coding the categorical variables in all the 3 sets \n",
    "x_train_encoded = pd.get_dummies(x_train, columns=categorical_columns) \n",
    "x_val_encoded = pd.get_dummies(x_val, columns=categorical_columns)\n",
    "x_test_encoded = pd.get_dummies(x_test, columns=categorical_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is aimed at computing the baseline results on the validation set. These results will be used as a reference for eventual improvements after parameters tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "           Predicted 0 Predicted 1\n",
      "  Actual 0       1804        397\n",
      "  Actual 1        388        343\n",
      "Recall: 0.469\n",
      "Precision: 0.464\n",
      "Accuracy: 0.7323\n",
      "F1 score: 0.4646\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train_encoded, y_train)\n",
    "pred = model.predict(x_val_encoded)\n",
    "evaluate(pred, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, hyperparameters tuning is applied. I have decided to use the 'grid search' method, which goes across all the possible combination of the sets of the given parameters. In this case, I have decided to test for the nodes division criterion, the maximum depth of the tree, and the minimum number of samples for a split, which consists of the minumum number of points that need to belong to the same classes before a node division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 15, 'min_samples_split': 40}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    \"criterion\":['gini', 'entropy'],\n",
    "    \"max_depth\":[10,15,20,30,35,40,45],\n",
    "    'min_samples_split': [20, 30, 40]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid = hyperparameters, cv = 10)\n",
    "\n",
    "grid_search.fit(x_train_encoded, y_train)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells is aimed at the training and evaluation (on the validation set) of the model with the best parameters found after HP optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "           Predicted 0 Predicted 1\n",
      "  Actual 0       1862        339\n",
      "  Actual 1        341        390\n",
      "Recall: 0.534\n",
      "Precision: 0.535\n",
      "Accuracy: 0.7681\n",
      "F1 score: 0.5347\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=15, min_samples_split=40)\n",
    "model.fit(x_train_encoded, y_train)\n",
    "pred = model.predict(x_val_encoded)\n",
    "evaluate(pred, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will try to apply a principal component analysis (PCA): this algorithm is aimed at dimensionality reduction with a parallel try to keep the variability of the predictor as high as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "           Predicted 0 Predicted 1\n",
      "  Actual 0       1829        372\n",
      "  Actual 1        493        238\n",
      "Recall: 0.326\n",
      "Precision: 0.39\n",
      "Accuracy: 0.705\n",
      "F1 score: 0.3753\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_encoded)\n",
    "x_val_scaled = scaler.fit_transform(x_val_encoded)\n",
    "\n",
    "# Applica la PCA sulle variabili dopo l'encoding\n",
    "pca = PCA(n_components=20)  # Sostituisci 'numero_di_componenti' con il numero desiderato di componenti\n",
    "x_train_pca = pca.fit_transform(x_train_scaled)\n",
    "x_val_pca = pca.fit_transform(x_val_scaled)\n",
    "\n",
    "# Fitting and evaluation of the model (with optimized HP) with predictors post PCA\n",
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=15, min_samples_split=40)\n",
    "model.fit(x_train_pca, y_train)\n",
    "pred = model.predict(x_val_pca)\n",
    "evaluate(pred, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will apply feature selection, with the 'SelectKBest' function. With a k = 15, this function will provide the 15 best predictors for this model. Like PCA, this activity is aimed at dimensionality reduction and it may lead to higher performances of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "           Predicted 0 Predicted 1\n",
      "  Actual 0       1927        274\n",
      "  Actual 1        393        338\n",
      "Recall: 0.462\n",
      "Precision: 0.552\n",
      "Accuracy: 0.7725\n",
      "F1 score: 0.5316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leo/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "selector = SelectKBest(score_func=f_classif, k=15)  # Sostituisci 'k' con il numero desiderato di caratteristiche\n",
    "x_selected = selector.fit_transform(x_train_encoded, y_train)\n",
    "\n",
    "selected_columns = selector.get_support()  # Print this value if you want to see the selected columns \n",
    "\n",
    "# Ottieni l'elenco delle colonne selezionate utilizzando l'array booleano\n",
    "all_columns = x_train_encoded.columns.tolist()  # Sostituisci con i nomi reali delle colonne\n",
    "selected_column_names = [column for column, selected in zip(all_columns, selected_columns) if selected]\n",
    "\n",
    "x_val_selected = x_val_encoded[selected_column_names]\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=15, min_samples_split=40)\n",
    "model.fit(x_selected, y_train)\n",
    "pred = model.predict(x_val_selected)\n",
    "evaluate(pred, y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now try to train the model (on the training set) on data without missing values. Of course, this can't be applied on the validation set (and eventually neither on the test set). \n",
    "The reason why I am not trying to apply imputations techniques on missing data, it's because we are dealing mostly with categorical data, and being the most reasonable way to impute missing categorical data the substitution with the mode, it wouldn't be a wise choice with this dataset. The reason is that this most of the variables of this dataset are imbalanced (there is a class which constitutes the majority of the data points for that column), and substituting missing data with the mode would lead to use almost always the majority class, and it wouldn't be realistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "           Predicted 0 Predicted 1\n",
      "  Actual 0       1977        224\n",
      "  Actual 1        466        265\n",
      "Recall: 0.363\n",
      "Precision: 0.542\n",
      "Accuracy: 0.7647\n",
      "F1 score: 0.4931\n"
     ]
    }
   ],
   "source": [
    "#REMOVE NA VALUES, TRAIN AND TEST \n",
    "\n",
    "\n",
    "x_train_clean = x_train.dropna()\n",
    "y_train_clean = y_train.loc[x_train_clean.index]\n",
    "\n",
    "x_train_clean_encoded = pd.get_dummies(x_train_clean, columns=categorical_columns) \n",
    "\n",
    "new_columns = x_train_clean_encoded.columns\n",
    "x_val_encoded = x_val_encoded[new_columns]\n",
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=15, min_samples_split=40)\n",
    "model.fit(x_train_clean_encoded, y_train_clean)\n",
    "pred = model.predict(x_val_encoded)\n",
    "evaluate(pred, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: \n",
    "\n",
    "Starting from the baseline result, where we obtained a 73% accuracy (73% out of all the data points classified correctly) and a 47% recall (47% of the positive class correctly classified), the best improvement has been obtained after hyperparameter optimization, with a 77% accuracy and a 53% recall.\n",
    "Applying PCA didn't lead to any improvement, while feature selection allowed an increase in just the accuracy.\n",
    "I will now evaluate the generalization capability of the model with the optimized hyperparameters on the test set. Before doing this, I will train the model on the combination of the validation and training set, for an eventual further improvement of the training \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "           Predicted 0 Predicted 1\n",
      "  Actual 0       1796        361\n",
      "  Actual 1        243        474\n",
      "Recall: 0.661\n",
      "Precision: 0.568\n",
      "Accuracy: 0.7898\n",
      "F1 score: 0.5842\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test set\n",
    "\n",
    "x_train_full_encoded = pd.get_dummies(x_train_temp, columns=categorical_columns) # X_train_temp was created at the first train \n",
    "                                                                                 # test split and comprehends also the actual validaiton set\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=15, min_samples_split=40)\n",
    "model.fit(x_train_full_encoded, y_train_temp)\n",
    "\n",
    "predicted_probabilities = model.predict_proba(X = x_test_encoded, check_input=True)  # This array will contained the predicted probabilities from the test set data\n",
    "\n",
    "\n",
    "pred = model.predict(x_test_encoded) # I will anyway compute the binary predictions for evaluation purposes\n",
    "evaluate(pred, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results on the validation set are even better than what we obtained before.\n",
    "This means that the model generalized pretty well, and also manage to have a reduction especially in the false negative errors (as the higher recall shows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explainability of the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
