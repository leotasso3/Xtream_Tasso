{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd \n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this binary prediction task, I have decided to use the decision tree classifier. The latter is indeed a 'white box' model, as it is highly interpretable and its functiong and choises for each data point can be deeply investigated and explained.\n",
    "Furthermore, the decision tree classifier is one of the best model for classification tasks. \n",
    "A decision tree is also capable of working with missing values, and this dataset contains a non-negligible number of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a custom function aimed at displayig the most importan metrics for a binary classification, together with a confusion matrix\n",
    "# The input for this function must be binary predictions\n",
    "\n",
    "def evaluate(y_pred, y_validation):\n",
    "    cm = confusion_matrix(y_validation, y_pred)\n",
    "    acc = accuracy_score(y_validation,y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    recall = tp / (tp + fn)\n",
    "    beta = 0.5\n",
    "\n",
    "    if tp == 0 and fp == 0:\n",
    "        precision = 0.0\n",
    "        F1 = 0.0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "        F1 = (1+beta**2)*((precision * recall)/((beta**2 * precision) + recall))\n",
    "        \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"{:>10} {:>10} {:>10}\".format(\"\", \"Predicted 0\", \"Predicted 1\"))\n",
    "    print(\"{:>10} {:>10} {:>10}\".format(\"Actual 0\", tn, fp))\n",
    "    print(\"{:>10} {:>10} {:>10}\".format(\"Actual 1\", fn, tp))\n",
    "    print(\"Recall:\", round(recall, 3))\n",
    "    print(\"Precision:\", round(precision, 3))\n",
    "    print(\"Accuracy:\", round(acc, 4))\n",
    "    print(\"F1 score:\", round(F1, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/leotasso3/Xtream_Tasso/main/datasets/employee-churn/churn.csv\")  # Reading the dataset\n",
    "\n",
    "# isolating the target variable, which is in the last column of the dataset\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# The predictors will be all the other variables, but for the city (explanation below) and the ID column \n",
    "x = df.iloc[:, 1:13]\n",
    "x=x.drop('city', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided to split the dataset into training (70%), validation (15%) and test set (15%). The training set will be indeed used for training the DT to build its nodes and leaves. \n",
    "I want to try different configuration of the model, starting from a baseline one with the (almost) untouched dataset, for then moving to hyperparameter optimization, PCA and feature selection. This is why the validation set is here. After training the model, I will test the performance of the latter on the validation data, and as I want to try different configuration, I will do it more than once. \n",
    "The metrics obtained on the validation set will be used for selecting the model configuration that yielded the best results. That model will then be tested on the test set: this will give the opportunity to evaluate the generalization of the model on unseen data.\n",
    "Without the validation set, testing different models configurations on the test set would could lead to overfitting, as doing this would basically mean to adapt the choice of the best model configuration on future data, which of course is not possible in reality. \n",
    "As the dataset is slightly imbalanced, I will apply stratification with respect to the target variable, so that the same proportion of the latter will be present in every set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione della variabile target nel train set: target\n",
      "0.0    0.750674\n",
      "1.0    0.249326\n",
      "Name: proportion, dtype: float64\n",
      "Distribuzione della variabile target nel validation set: target\n",
      "0.0    0.750682\n",
      "1.0    0.249318\n",
      "Name: proportion, dtype: float64\n",
      "Distribuzione della variabile target nel test set: target\n",
      "0.0    0.750522\n",
      "1.0    0.249478\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Set creation and stratification (the randon_state is for reproducibility purposes)\n",
    "x_train_temp, x_test, y_train_temp, y_test = train_test_split(x, y, test_size=0.15, random_state=42, stratify=y)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_temp, y_train_temp, test_size=0.18, random_state=42, stratify=y_train_temp)\n",
    "\n",
    "\n",
    "# Verification of stratification\n",
    "print(\"Distribution of the target variable in the training set:\", y_train.value_counts(normalize=True))\n",
    "print(\"Distribution of the target variable in the validation set:\", y_val.value_counts(normalize=True))\n",
    "print(\"Distribution of the target variable in the test set:\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the DecisionTreeClassifier won't work with 'object' as variable type (categorical), I will apply dummy encoding.\n",
    "This will increase the dimensionality of the dataset and could therefore increment the computational costs for the model training.\n",
    "For an eventual future dataset way larger than the one provided here, it is advisable to apply PCA or feature selection (which will in any case tested on this dataset as well).\n",
    "The reasons why I have decided to remove the 'city' column are mainly 2: \n",
    "-This variable has 73 classes, and dummy coding it would result in a severe increase in the dimensionality of the dataset, which of course is not positive either for computational costs and risk of overfitting\n",
    "-Stratify this variable could be very difficult: if there are very few people coming from one city, it could be that the relative coded column won't be present in all the 3 sets, which of course will lead to an error of model training or model prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = x_train.select_dtypes(include=['object']).columns.tolist() # Storing the categorical variable\n",
    "\n",
    "# Dummy coding the categorical variables in all the 3 sets \n",
    "x_train_encoded = pd.get_dummies(x_train, columns=categorical_columns) \n",
    "x_val_encoded = pd.get_dummies(x_val, columns=categorical_columns)\n",
    "x_test_encoded = pd.get_dummies(x_test, columns=categorical_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is aimed at computing the baseline results on the validation set. These results will be used as a reference for eventual improvements after parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train_encoded, y_train)\n",
    "pred = model.predict(x_val_encoded)\n",
    "evaluate(pred, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [10, 15, 20, 30, 35, 40, 45],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10, 20]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [10, 15, 20, 30, 35, 40, 45],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10, 20]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [10, 15, 20, 30, 35, 40, 45],\n",
       "                         'min_samples_split': [2, 5, 10, 20]})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    \"criterion\":['gini', 'entropy'],\n",
    "    \"max_depth\":[10,15,20,30,35,40,45],\n",
    "    'min_samples_split': [2, 5, 10, 20]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid = hyperparameters, cv = 10)\n",
    "\n",
    "grid_search.fit(x_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 15, 'min_samples_split': 20}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "           Predicted 0 Predicted 1\n",
      "  Actual 0       1843        358\n",
      "  Actual 1        329        402\n",
      "Recall: 0.55\n",
      "Precision: 0.529\n",
      "Accuracy: 0.7657\n",
      "F1 score: 0.533\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=15, min_samples_split=20)\n",
    "model.fit(x_train_encoded, y_train)\n",
    "pred = model.predict(x_val_encoded)\n",
    "evaluate(pred, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "           Predicted 0 Predicted 1\n",
      "  Actual 0       1663        538\n",
      "  Actual 1        490        241\n",
      "Recall: 0.33\n",
      "Precision: 0.309\n",
      "Accuracy: 0.6494\n",
      "F1 score: 0.3132\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_encoded)\n",
    "x_val_scaled = scaler.fit_transform(x_val_encoded)\n",
    "\n",
    "# Applica la PCA sulle variabili dopo l'encoding\n",
    "pca = PCA(n_components=20)  # Sostituisci 'numero_di_componenti' con il numero desiderato di componenti\n",
    "x_train_pca = pca.fit_transform(x_train_scaled)\n",
    "x_val_pca = pca.fit_transform(x_val_scaled)\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=15, min_samples_split=20)\n",
    "model.fit(x_train_pca, y_train)\n",
    "pred = model.predict(x_val_pca)\n",
    "evaluate(pred, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying selecting k-best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "           Predicted 0 Predicted 1\n",
      "  Actual 0       1966        235\n",
      "  Actual 1        421        310\n",
      "Recall: 0.424\n",
      "Precision: 0.569\n",
      "Accuracy: 0.7763\n",
      "F1 score: 0.5325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leo/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "selector = SelectKBest(score_func=f_classif, k=15)  # Sostituisci 'k' con il numero desiderato di caratteristiche\n",
    "x_selected = selector.fit_transform(x_train_encoded, y_train)\n",
    "\n",
    "selected_columns = selector.get_support()\n",
    "\n",
    "# Ottieni l'elenco delle colonne selezionate utilizzando l'array booleano\n",
    "all_columns = x_train_encoded.columns.tolist()  # Sostituisci con i nomi reali delle colonne\n",
    "selected_column_names = [column for column, selected in zip(all_columns, selected_columns) if selected]\n",
    "\n",
    "x_val_selected = x_val_encoded[selected_column_names]\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=15, min_samples_split=20)\n",
    "model.fit(x_selected, y_train)\n",
    "pred = model.predict(x_val_selected)\n",
    "evaluate(pred, y_val)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
